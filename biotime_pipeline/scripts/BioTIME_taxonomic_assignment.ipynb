{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9e16aaf",
      "metadata": {
        "id": "d9e16aaf"
      },
      "source": [
        "# Taxonomic Assignment of Species\n",
        "This script takes biotime_unique_entries.csv as input, assigns taxonomic information for each species, and outputs a taxonomy key. It relies on Biopython and calls upon the NCBI, COL, GBIF, and WORMS databases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrrHbca8zLWx",
      "metadata": {
        "id": "lrrHbca8zLWx"
      },
      "source": [
        "# Library and API Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "kBDJ-NrRXViX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBDJ-NrRXViX",
        "outputId": "b82ffaaa-a4de-4dd6-d6f3-a69d29b9e503"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import shutil  \n",
        "import glob\n",
        "import json \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "# Biopython for NCBI taxonomy\n",
        "from Bio import Entrez\n",
        "\n",
        "# CLI progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# NCBI credentials\n",
        "Entrez.email = \"emduggan@mit.edu\"\n",
        "Entrez.api_key = \"2e5155aba559345711a3af676cb6c6703608\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRn4IJJ5zfaU",
      "metadata": {
        "id": "rRn4IJJ5zfaU"
      },
      "source": [
        "# Defining functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bvbqxn05byoo",
      "metadata": {
        "id": "bvbqxn05byoo"
      },
      "outputs": [],
      "source": [
        "_warned_inputs = set()  # to suppress repeated genus-only warnings\n",
        "\n",
        "def save_cache(filename, new_cache):\n",
        "    # Load existing cache if available\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, \"r\") as f:\n",
        "            try:\n",
        "                existing_cache = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                existing_cache = {}\n",
        "    else:\n",
        "        existing_cache = {}\n",
        "\n",
        "    # Merge: only update new keys or changed values\n",
        "    existing_cache.update(new_cache)\n",
        "\n",
        "    # Write back merged cache\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(existing_cache, f)\n",
        "\n",
        "def load_cache(filename):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename) as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "\n",
        "def extract(lineage, rank):\n",
        "    \"\"\"Extract the name for a given rank from a lineage list.\"\"\"\n",
        "    for entry in lineage:\n",
        "        if entry.get(\"Rank\", \"\").lower() == rank.lower():\n",
        "            return entry.get(\"ScientificName\")\n",
        "        if entry.get(\"rank\", \"\").lower() == rank.lower():\n",
        "            return entry.get(\"name\")\n",
        "    return None\n",
        "\n",
        "def ncbi_query(term):\n",
        "    \"\"\"\n",
        "    Query NCBI Taxonomy for full lineage.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Lineage dict, or None if not found.\n",
        "    \"\"\"\n",
        "    term = term.strip().title()\n",
        "    if term in ncbi_cache:\n",
        "        return ncbi_cache[term]\n",
        "    try:\n",
        "        search = Entrez.esearch(db=\"taxonomy\", term=term, retmode=\"xml\")\n",
        "        result = Entrez.read(search)\n",
        "        if not result[\"IdList\"]:\n",
        "            return None\n",
        "\n",
        "        taxid = result[\"IdList\"][0]\n",
        "        fetch = Entrez.efetch(db=\"taxonomy\", id=taxid, retmode=\"xml\")\n",
        "        record = Entrez.read(fetch)[0]\n",
        "        lineage = record.get(\"LineageEx\", [])\n",
        "\n",
        "        result_lineage = {\n",
        "            \"kingdom\": extract(lineage, \"kingdom\"),\n",
        "            \"phylum\":  extract(lineage, \"phylum\"),\n",
        "            \"class\":   extract(lineage, \"class\"),\n",
        "            \"order\":   extract(lineage, \"order\"),\n",
        "            \"family\":  extract(lineage, \"family\"),\n",
        "            \"genus\":   extract(lineage, \"genus\"),\n",
        "            \"source\":  \"ncbi\"\n",
        "        }\n",
        "\n",
        "        if not any(result_lineage.values()):\n",
        "            return None\n",
        "\n",
        "        ncbi_cache[term] = result_lineage\n",
        "        return result_lineage\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "#TODO make it so that fuzzy-matching on common names does not occur\n",
        "def col_query(term):\n",
        "    \"\"\"\n",
        "    Query Catalogue of Life (COL) API for lineage. Rejects fuzzy/common name matches.\n",
        "\n",
        "    Args:\n",
        "        term (str): Genus or species name.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Lineage dictionary if matched exactly; else None.\n",
        "    \"\"\"\n",
        "    term = term.strip().title()\n",
        "    if term in col_cache:\n",
        "        return col_cache[term]\n",
        "\n",
        "    url = f\"https://api.catalogueoflife.org/nameusage/search?q={term}\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data['total'] == 0 or not data['result']:\n",
        "            return None\n",
        "\n",
        "        result = data['result'][0]\n",
        "        matched_name = result.get('scientificName', '').strip().title()\n",
        "        match_type = result.get('matchType', '').upper()\n",
        "\n",
        "        # Strict match: name must match exactly AND be an exact match type\n",
        "        if matched_name != term or match_type not in {\"EXACT\", \"EXACT_NAME\"}:\n",
        "            return None\n",
        "\n",
        "        lineage = result.get('classification', [])\n",
        "\n",
        "        result_lineage = {\n",
        "            \"kingdom\": extract(lineage, \"kingdom\"),\n",
        "            \"phylum\":  extract(lineage, \"phylum\"),\n",
        "            \"class\":   extract(lineage, \"class\"),\n",
        "            \"order\":   extract(lineage, \"order\"),\n",
        "            \"family\":  extract(lineage, \"family\"),\n",
        "            \"genus\":   extract(lineage, \"genus\"),\n",
        "            \"source\":  \"col\"\n",
        "        }\n",
        "\n",
        "        if not any(result_lineage.values()):\n",
        "            return None\n",
        "\n",
        "        col_cache[term] = result_lineage\n",
        "        return result_lineage\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def gbif_query(term):\n",
        "    \"\"\"\n",
        "    Query GBIF species match API for lineage.\n",
        "\n",
        "    Args:\n",
        "        term (str): Genus or species name.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Lineage dict if found and valid, else None.\n",
        "    \"\"\"\n",
        "    term = term.strip().title()\n",
        "    if term in gbif_cache:\n",
        "        return gbif_cache[term]\n",
        "\n",
        "    url = f\"https://api.gbif.org/v1/species/match?name={term}\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Reject results without a confident match\n",
        "        if data.get(\"matchType\") == \"NONE\" or data.get(\"confidence\", 0) < 90:\n",
        "            return None\n",
        "\n",
        "        result_lineage = {\n",
        "            \"kingdom\": data.get(\"kingdom\"),\n",
        "            \"phylum\":  data.get(\"phylum\"),\n",
        "            \"class\":   data.get(\"class\"),\n",
        "            \"order\":   data.get(\"order\"),\n",
        "            \"family\":  data.get(\"family\"),\n",
        "            \"genus\":   data.get(\"genus\"),  # no fallback\n",
        "            \"source\":  \"gbif\"\n",
        "        }\n",
        "\n",
        "        gbif_cache[term] = result_lineage\n",
        "        return result_lineage\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def worms_query(term):\n",
        "    \"\"\"\n",
        "    Query WoRMS (World Register of Marine Species) for lineage.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Lineage dict, or None if not found.\n",
        "    \"\"\"\n",
        "    term = term.strip().title()\n",
        "    if term in worms_cache:\n",
        "        return worms_cache[term]\n",
        "    url = f\"http://www.marinespecies.org/rest/AphiaRecordsByName/{term}?like=false&marine_only=false\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if not data or not isinstance(data, list):\n",
        "            return None\n",
        "\n",
        "        entry = data[0]\n",
        "        result_lineage = {\n",
        "            \"kingdom\": entry.get(\"kingdom\"),\n",
        "            \"phylum\":  entry.get(\"phylum\"),\n",
        "            \"class\":   entry.get(\"class\"),\n",
        "            \"order\":   entry.get(\"order\"),\n",
        "            \"family\":  entry.get(\"family\"),\n",
        "            \"genus\":   entry.get(\"genus\"),\n",
        "            \"source\":  \"worms\"\n",
        "        }\n",
        "\n",
        "        if not any(result_lineage.values()):\n",
        "            return None\n",
        "\n",
        "        worms_cache[term] = result_lineage\n",
        "        return result_lineage\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def query_taxonomy_lineage(term, allow_genus_fallback=True, db_order=None, raise_errors=True):\n",
        "    \"\"\"\n",
        "    Query taxonomy hierarchy using NCBI, then fallback to COL, GBIF, WoRMS.\n",
        "\n",
        "    Args:\n",
        "        term (str): Binomial or genus name (e.g., \"Panthera leo\" or \"Panthera\").\n",
        "        allow_genus_fallback (bool): Whether to try genus-level lookup if species fails.\n",
        "        db_order (list[str]): Priority order of databases to query.\n",
        "        raise_errors (bool): If True, raise query exceptions; else warn and continue.\n",
        "\n",
        "    Returns:\n",
        "        dict: Taxonomic lineage with source label. Fields may be None.\n",
        "    \"\"\"\n",
        "    original_term = term  # store before any cleanup\n",
        "    term = term.strip().title()\n",
        "    parts = term.split()\n",
        "    genus = parts[0]\n",
        "\n",
        "    # Only warn for genus-only original inputs\n",
        "    warning_key = original_term.lower()\n",
        "    if len(parts) < 2 and warning_key not in _warned_inputs:\n",
        "        warnings.warn(f\"Input '{original_term}' looks like a genus-only name. Only genus-level query will be attempted.\")\n",
        "        _warned_inputs.add(warning_key)\n",
        "\n",
        "    db_order = db_order or [\"ncbi\", \"gbif\", \"worms\", \"col\"]\n",
        "    query_funcs = {\n",
        "        \"ncbi\": ncbi_query,\n",
        "        \"col\": col_query,\n",
        "        \"gbif\": gbif_query,\n",
        "        \"worms\": worms_query}\n",
        "\n",
        "    for db in db_order:\n",
        "        if db not in query_funcs:\n",
        "            raise ValueError(f\"Unrecognized database: '{db}'\")\n",
        "\n",
        "        fn = query_funcs[db]\n",
        "\n",
        "        try:\n",
        "            # Try species-level query\n",
        "            if len(parts) >= 2:\n",
        "                result = fn(term)\n",
        "                if result and result.get(\"class\"):\n",
        "                    return result\n",
        "\n",
        "            # Try genus-level fallback\n",
        "            if allow_genus_fallback:\n",
        "                genus_result = fn(genus)\n",
        "                if genus_result and genus_result.get(\"class\"):\n",
        "                    if \"(genus-level)\" not in genus_result[\"source\"]:\n",
        "                        genus_result[\"source\"] += \" (genus-level)\"\n",
        "                    if not genus_result.get(\"genus\"):\n",
        "                        genus_result[\"genus\"] = genus\n",
        "                    return genus_result\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = f\"Failed querying {db} for '{term}': {e}\"\n",
        "            if raise_errors:\n",
        "                raise RuntimeError(msg) from e\n",
        "            # warnings.warn(msg)\n",
        "            continue\n",
        "\n",
        "    # Total failure — return all Nones including genus\n",
        "    return {\n",
        "        \"kingdom\": None, \"phylum\": None, \"class\": None,\n",
        "        \"order\": None, \"family\": None, \"genus\": None,\n",
        "        \"source\": \"not_found\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc104ce6",
      "metadata": {},
      "source": [
        "# Creating shared session and caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65081457",
      "metadata": {},
      "outputs": [],
      "source": [
        "# species list\n",
        "df = pd.read_csv(\"../data/biotime_unique_entries.csv\")\n",
        "species_list = df[\"genus_species\"].dropna().str.strip().str.title().unique()\n",
        "\n",
        "# Create shared session for reuse\n",
        "session = requests.Session()\n",
        "timeout = 10  # seconds\n",
        "\n",
        "# create cache subfolder\n",
        "os.makedirs(\"../data/cache\", exist_ok=True)\n",
        "\n",
        "# Load persistent caches\n",
        "ncbi_cache = load_cache(\"../data/cache/ncbi_cache.json\")\n",
        "col_cache = load_cache(\"../data/cache/col_cache.json\")\n",
        "gbif_cache = load_cache(\"../data/cache/gbif_cache.json\")\n",
        "worms_cache = load_cache(\"../data/cache/worms_cache.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Se85TdQFzs_x",
      "metadata": {
        "id": "Se85TdQFzs_x"
      },
      "source": [
        "# Run Taxonomic Assignment with Batching and Caching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3380c570",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 already fully processed. Skipping.\n",
            "Processing batch 2/5 (8000 species)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [14:07:43<00:00,  6.36s/it]         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved batch 2 to ../data/cache/classified_batch_002.csv\n",
            "Processing batch 3/5 (8000 species)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [33:51<00:00,  3.94it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved batch 3 to ../data/cache/classified_batch_003.csv\n",
            "Processing batch 4/5 (8000 species)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [34:04<00:00,  3.91it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved batch 4 to ../data/cache/classified_batch_004.csv\n",
            "Processing batch 5/5 (7056 species)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7056/7056 [29:06<00:00,  4.04it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved batch 5 to ../data/cache/classified_batch_005.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8000\n",
        "flush_interval = 1000\n",
        "num_batches = math.ceil(len(species_list) / batch_size)\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = start + batch_size\n",
        "    batch = species_list[start:end]\n",
        "\n",
        "    output_path = f\"../data/cache/classified_batch_{i+1:03}.csv\"\n",
        "\n",
        "    # Resume support: skip already-processed species in batch\n",
        "    if os.path.exists(output_path):\n",
        "        try:\n",
        "            processed = pd.read_csv(output_path)[\"genus_species\"]\n",
        "            if len(processed) >= len(batch):\n",
        "                print(f\"Batch {i+1} already fully processed. Skipping.\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Batch {i+1} partially processed. Resuming unfinished entries.\")\n",
        "                processed_species = set(processed)\n",
        "        except Exception:\n",
        "            print(f\"Could not read existing batch {i+1}. Reprocessing from scratch.\")\n",
        "            processed_species = set()\n",
        "    else:\n",
        "        processed_species = set()\n",
        "\n",
        "\n",
        "    print(f\"Processing batch {i+1}/{num_batches} ({len(batch)} species)\")\n",
        "\n",
        "    buffer = []\n",
        "    for j, sp in enumerate(tqdm(batch)):\n",
        "        if sp in processed_species:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = query_taxonomy_lineage(sp)\n",
        "        except Exception as e:\n",
        "            result = {\"genus_species\": sp, \"error\": str(e)}\n",
        "        result[\"genus_species\"] = sp\n",
        "        buffer.append(result)\n",
        "\n",
        "        if (j + 1) % flush_interval == 0 or (j + 1) == len(batch):\n",
        "            df = pd.DataFrame(buffer)\n",
        "            df.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
        "            buffer.clear()\n",
        "\n",
        "            save_cache(\"../data/cache/ncbi_cache.json\", ncbi_cache)\n",
        "            save_cache(\"../data/cache/col_cache.json\", col_cache)\n",
        "            save_cache(\"../data/cache/gbif_cache.json\", gbif_cache)\n",
        "            save_cache(\"../data/cache/worms_cache.json\", worms_cache)\n",
        "\n",
        "    print(f\"Saved batch {i+1} to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EB3j9upSz_Fo",
      "metadata": {
        "id": "EB3j9upSz_Fo"
      },
      "source": [
        "# Exporting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f8876b13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging all classified batches...\n",
            "Exported classified and unclassified results.\n",
            "Total species processed:     39,556\n",
            "Classified species:          39,534\n",
            "Unclassified species:        22\n"
          ]
        }
      ],
      "source": [
        "# --- Step 1: Merge all batch CSVs ---\n",
        "print(\"Merging all classified batches...\")\n",
        "batch_files = sorted(glob.glob(\"../data/cache/classified_batch_*.csv\"))\n",
        "df_all = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
        "\n",
        "# --- Step 2: Separate classified and unclassified ---\n",
        "# Use \"class\" as the required indicator of a successful classification\n",
        "required_fields = [\"class\"]  # can expand to include \"kingdom\", \"phylum\", etc.\n",
        "\n",
        "# Successful: all required fields are not null\n",
        "df_classified = df_all[df_all[required_fields].notna().all(axis=1)]\n",
        "\n",
        "# Unsuccessful: all required fields are null or there's an error field\n",
        "if \"error\" in df_all.columns:\n",
        "    df_unclassified = df_all[\n",
        "        df_all[required_fields].isna().all(axis=1) | df_all[\"error\"].notnull()\n",
        "    ]\n",
        "else:\n",
        "    df_unclassified = df_all[df_all[required_fields].isna().all(axis=1)]\n",
        "\n",
        "# --- Step 3: Export ---\n",
        "df_classified.to_csv(\"../data/classified_species.csv\", index=False)\n",
        "df_unclassified.to_csv(\"../data/unclassified_species.csv\", index=False)\n",
        "\n",
        "# --- Step 4: Summary ---\n",
        "print(\"Exported classified and unclassified results.\")\n",
        "print(f\"Total species processed:     {len(df_all):,}\")\n",
        "print(f\"Classified species:          {len(df_classified):,}\")\n",
        "print(f\"Unclassified species:        {len(df_unclassified):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3920393a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'kingdom': None, 'phylum': None, 'class': None, 'order': None, 'family': None, 'genus': None, 'source': 'not_found'}\n"
          ]
        }
      ],
      "source": [
        "print(query_taxonomy_lineage(\"Barentsia benedeni\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "biodiversity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
