{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9e16aaf",
      "metadata": {
        "id": "d9e16aaf"
      },
      "source": [
        "# Genus/Species to Class Converter\n",
        "This notebook contains code to convert the genus/species listing from the BioTIME-DB to taxonomic class using Biopython and calling upon the NCBI, COL, GBIF, and WORMS databases, in that order."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Loop Package and Key Setup"
      ],
      "metadata": {
        "id": "lrrHbca8zLWx"
      },
      "id": "lrrHbca8zLWx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages and set NCBI API information.\n",
        "!pip install biopython tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from Bio import Entrez\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from getpass import getpass\n",
        "\n",
        "! wget https://raw.githubusercontent.com/emd-aquila/cs3-biodiversity/main/data/unique_genus_species.csv -O myfile.csv\n",
        "df_all = pd.read_csv(\"myfile.csv\")\n",
        "\n",
        "Entrez.email = \"emduggan@mit.edu\"\n",
        "Entrez.api_key = \"2e5155aba559345711a3af676cb6c6703608\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBDJ-NrRXViX",
        "outputId": "ef3b7313-3c57-438e-a1ad-115c57861f13"
      },
      "id": "kBDJ-NrRXViX",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "--2025-05-08 21:30:28--  https://raw.githubusercontent.com/emd-aquila/cs3-biodiversity/main/data/unique_genus_species.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1185895 (1.1M) [text/plain]\n",
            "Saving to: ‘myfile.csv’\n",
            "\n",
            "myfile.csv          100%[===================>]   1.13M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-05-08 21:30:29 (19.5 MB/s) - ‘myfile.csv’ saved [1185895/1185895]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access the private SSH key to upload to GitHub"
      ],
      "metadata": {
        "id": "ZNrK3lB94gu3"
      },
      "id": "ZNrK3lB94gu3"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload id_ed25519 --> under Users/user/.ssh\n",
        "!mkdir -p ~/.ssh\n",
        "!mv id_ed25519 ~/.ssh/\n",
        "!chmod 600 ~/.ssh/id_ed25519\n",
        "!ssh-agent bash -c 'ssh-add ~/.ssh/id_ed25519; ssh -T git@github.com'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XSsWaR923nFR",
        "outputId": "4b946b41-7c0c-41af-e2bc-0f8fbc6fffa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "id": "XSsWaR923nFR",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ff21088-3295-4a3a-9bcd-8652773f5894\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ff21088-3295-4a3a-9bcd-8652773f5894\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving id_ed25519 to id_ed25519\n",
            "Identity added: /root/.ssh/id_ed25519 (emduggan@mit.edu)\n",
            "Hi emd-aquila! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create local clone of GitHub repository for file storage"
      ],
      "metadata": {
        "id": "OuwkOnoIzZC6"
      },
      "id": "OuwkOnoIzZC6"
    },
    {
      "cell_type": "code",
      "source": [
        "# if needed to clear existing clone\n",
        "!rm -rf cs3-biodiversity\n",
        "\n",
        "# get access code and clone repo\n",
        "os.environ[\"GITHUB_TOKEN\"] = getpass(\"🔐 Enter your GitHub token: \")\n",
        "token = os.environ[\"GITHUB_TOKEN\"]\n",
        "repo_url = f\"https://emd-aquila:{token}@github.com/emd-aquila/cs3-biodiversity.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcjW9I7KuPMr",
        "outputId": "b20fd19a-b6a0-45e2-97f0-4939f2973c85"
      },
      "id": "JcjW9I7KuPMr",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔐 Enter your GitHub token: ··········\n",
            "Cloning into 'cs3-biodiversity'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 73 (delta 33), reused 24 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (73/73), 679.31 KiB | 10.45 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining functions to query database APIs and search for species"
      ],
      "metadata": {
        "id": "rRn4IJJ5zfaU"
      },
      "id": "rRn4IJJ5zfaU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses an email and API key to query the NCBI database\n",
        "def ncbi_query(term):\n",
        "    try:\n",
        "        search = Entrez.esearch(db=\"taxonomy\", term=term, retmode=\"xml\")\n",
        "        result = Entrez.read(search)\n",
        "        if result[\"IdList\"]:\n",
        "            taxid = result[\"IdList\"][0]\n",
        "            fetch = Entrez.efetch(db=\"taxonomy\", id=taxid, retmode=\"xml\")\n",
        "            record = Entrez.read(fetch)[0]\n",
        "            lineage = record.get(\"LineageEx\", [])\n",
        "            class_entry = next((r for r in lineage if r.get(\"Rank\") == \"class\"), None)\n",
        "            return class_entry[\"ScientificName\"] if class_entry else None\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ NCBI lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the Categories of Life database\n",
        "def col_query(term):\n",
        "    url = f\"https://api.catalogueoflife.org/nameusage/search?q={term}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data['total'] > 0:\n",
        "            result = data['result'][0]\n",
        "            lineage = result.get('classification', [])\n",
        "            class_entry = next((r for r in lineage if r.get('rank') == 'class'), None)\n",
        "            return class_entry['name'] if class_entry else None\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ COL lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the WoRMS database\n",
        "def worms_query(term):\n",
        "    url = f\"http://www.marinespecies.org/rest/AphiaRecordsByName/{term}?like=false&marine_only=false\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data:\n",
        "            return data[0].get('class')\n",
        "    except Exception as e:\n",
        "        print(f\"❌ WoRMS lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the GBIF Database\n",
        "def gbif_query(term):\n",
        "    url = f\"https://api.gbif.org/v1/species/match?name={term}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data.get(\"class\"):\n",
        "            return data[\"class\"]\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GBIF lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "bvbqxn05byoo"
      },
      "id": "bvbqxn05byoo",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Searches for the species in the specified DB, first by species and then by genus\n",
        "def search_term(scientific_name, db_type):\n",
        "    \"\"\"\n",
        "    Search for the taxonomic class of a species in a specified database.\n",
        "\n",
        "    Args:\n",
        "        scientific_name (str): The full species name to search.\n",
        "        db_type (str): One of 'NCBI', 'COL', 'WORMS', 'GBIF'.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The class name if found, else None.\n",
        "    \"\"\"\n",
        "\n",
        "    # 🔍 First try full species name\n",
        "    if db_type.upper() == 'NCBI':\n",
        "        result = ncbi_query(scientific_name)\n",
        "    elif db_type.upper() == 'COL':\n",
        "        result = col_query(scientific_name)\n",
        "    elif db_type.upper() == 'GBIF':\n",
        "        result = gbif_query(scientific_name)\n",
        "    elif db_type.upper() == 'WORMS':\n",
        "        result = worms_query(scientific_name)\n",
        "    else:\n",
        "        raise ValueError(\"db_type must be one of 'NCBI', 'COL', 'WORMS', or 'GBIF'.\")\n",
        "\n",
        "    if result:\n",
        "        return result\n",
        "\n",
        "    # 🔄 Fallback: try genus only\n",
        "    genus = scientific_name.split()[0]\n",
        "    if genus != scientific_name:\n",
        "        # print(f\"🔄 Fallback to genus: {genus}\")\n",
        "        if db_type.upper() == 'NCBI':\n",
        "            return ncbi_query(genus)\n",
        "        elif db_type.upper() == 'COL':\n",
        "            return col_query(genus)\n",
        "        elif db_type.upper() == 'WORMS':\n",
        "            return worms_query(genus)\n",
        "        elif db_type.upper() == 'GBIF':\n",
        "            return gbif_query(genus)\n",
        "    return None"
      ],
      "metadata": {
        "id": "No0o6J-liG8P"
      },
      "id": "No0o6J-liG8P",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_ssh_and_push(output_filename,\n",
        "                                 repo_name=\"cs3-biodiversity\",\n",
        "                                 subfolder=\"biotimes-with-class-label\",\n",
        "                                 github_username=\"emd-aquila\",\n",
        "                                 github_email=\"emduggan@mit.edu\",\n",
        "                                 github_name=\"Eli Duggan\"):\n",
        "    \"\"\"\n",
        "    Set up SSH (if needed) and push a batch output file to a subfolder in your GitHub repo using SSH.\n",
        "\n",
        "    Args:\n",
        "        output_filename (str): The CSV filename to push (already saved locally).\n",
        "        repo_name (str): Local folder name of the GitHub repo.\n",
        "        subfolder (str): Subfolder inside the repo to store the file.\n",
        "        github_username (str): Your GitHub username.\n",
        "        github_email (str): Your GitHub email.\n",
        "        github_name (str): Your full name for git config.\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ Add GitHub to known_hosts to prevent verification errors\n",
        "    print(\"🔑 Adding GitHub.com to known_hosts (if not already added)...\")\n",
        "    !mkdir -p ~/.ssh\n",
        "    !ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "\n",
        "    # ✅ Check if repo folder exists, clone if needed\n",
        "    if not os.path.exists(repo_name):\n",
        "        print(f\"📥 Repo folder '{repo_name}' not found. Cloning with SSH...\")\n",
        "        !git clone git@github.com:{github_username}/{repo_name}.git\n",
        "    else:\n",
        "        print(f\"✅ Repo folder '{repo_name}' already exists.\")\n",
        "\n",
        "    # ✅ Create the subfolder inside the repo if it doesn't exist\n",
        "    subfolder_path = f\"{repo_name}/{subfolder}\"\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Copy the batch result into the subfolder\n",
        "    shutil.copy(output_filename, f\"{subfolder_path}/{output_filename}\")\n",
        "    # print(f\"✅ Copied {output_filename} into {subfolder_path}/\")\n",
        "\n",
        "    # ✅ Push to GitHub\n",
        "    %cd {repo_name}\n",
        "\n",
        "    # Set Git config (only needed once)\n",
        "    !git config user.email \"{github_email}\"\n",
        "    !git config user.name \"{github_name}\"\n",
        "\n",
        "    # Set remote to SSH (just in case)\n",
        "    !git remote set-url origin git@github.com:{github_username}/{repo_name}.git\n",
        "\n",
        "    # Add, commit, push\n",
        "    !git add {subfolder}/{output_filename}\n",
        "    !git commit -m \"Add batch output {output_filename} to {subfolder}/\" || echo \"No changes to commit.\"\n",
        "    !git push origin main\n",
        "\n",
        "    # ✅ Return to root directory\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "c2txzoCM49oD"
      },
      "id": "c2txzoCM49oD",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_combined_csv_to_github(combined_filename=\"all_species_with_class.csv\",\n",
        "                                repo_name=\"cs3-biodiversity\",\n",
        "                                subfolder=\"biotimes-with-class-label\",\n",
        "                                github_username=\"emd-aquila\",\n",
        "                                github_email=\"emduggan@mit.edu\",\n",
        "                                github_name=\"Eli Duggan\"):\n",
        "    \"\"\"\n",
        "    Push the combined CSV file to a subfolder in your GitHub repo using SSH.\n",
        "    \"\"\"\n",
        "    # ✅ Build the full subfolder path\n",
        "    subfolder_path = f\"{repo_name}/{subfolder}\"\n",
        "\n",
        "    # ✅ Make sure the subfolder exists inside the repo\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Move the combined CSV into the subfolder\n",
        "    dest_path = f\"{subfolder_path}/{combined_filename}\"\n",
        "    if not os.path.exists(dest_path):\n",
        "        shutil.move(combined_filename, dest_path)\n",
        "        print(f\"✅ Moved {combined_filename} into {subfolder_path}/\")\n",
        "    else:\n",
        "        print(f\"✅ {combined_filename} already exists in {subfolder_path}/\")\n",
        "\n",
        "    # ✅ Push to GitHub\n",
        "    %cd {repo_name}\n",
        "\n",
        "    !git config user.email \"{github_email}\"\n",
        "    !git config user.name \"{github_name}\"\n",
        "    !git remote set-url origin git@github.com:{github_username}/{repo_name}.git\n",
        "\n",
        "    !git add {subfolder}/{combined_filename}\n",
        "    !git commit -m \"Add combined species class CSV to {subfolder}/\" || echo \"No changes to commit.\"\n",
        "    !git push origin main\n",
        "\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "R4cS-WGh-Jt2"
      },
      "id": "R4cS-WGh-Jt2",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching Data and Loading the Cache"
      ],
      "metadata": {
        "id": "_6IvUjjRzmo6"
      },
      "id": "_6IvUjjRzmo6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching data and saving\n"
      ],
      "metadata": {
        "id": "Se85TdQFzs_x"
      },
      "id": "Se85TdQFzs_x"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "num_batches = math.ceil(len(df_all) / batch_size)\n",
        "\n",
        "# Save each batch\n",
        "for i in range(num_batches):\n",
        "    batch_df = df_all.iloc[i*batch_size : (i+1)*batch_size]\n",
        "    batch_file = f\"species_batch_{i+1:03d}.csv\"\n",
        "    batch_df.to_csv(batch_file, index=False)\n",
        "    # print(f\"Saved {batch_file}\")"
      ],
      "metadata": {
        "id": "3UWDlYsyiyMU",
        "collapsed": true
      },
      "id": "3UWDlYsyiyMU",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Batch of Interest"
      ],
      "metadata": {
        "id": "B9ZkU8G0399J"
      },
      "id": "B9ZkU8G0399J"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_filename = \"species_batch_002.csv\""
      ],
      "metadata": {
        "id": "U-t-m9R639FA"
      },
      "id": "U-t-m9R639FA",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear cache (if needed)"
      ],
      "metadata": {
        "id": "wpjplRDuz6G0"
      },
      "id": "wpjplRDuz6G0"
    },
    {
      "cell_type": "code",
      "source": [
        "class_cache_file = batch_filename.replace(\".csv\", \"_class_cache.csv\")\n",
        "\n",
        "# Delete class cache\n",
        "if os.path.exists(class_cache_file):\n",
        "    os.remove(class_cache_file)\n",
        "    print(f\"🗑️ Deleted {class_cache_file}\")\n",
        "else:\n",
        "    print(f\"⚠️ No cache file found for {class_cache_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4jvsfJures6",
        "outputId": "6ddcd4c9-128e-4159-f3d5-02d78ae2af93"
      },
      "id": "Q4jvsfJures6",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗑️ Deleted species_batch_002_class_cache.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading batch files and caching files"
      ],
      "metadata": {
        "id": "lF0gQd8Wzwlt"
      },
      "id": "lF0gQd8Wzwlt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache files\n",
        "taxid_cache_file = batch_filename.replace(\".csv\", \"_taxid_cache.csv\")\n",
        "class_cache_file = batch_filename.replace(\".csv\", \"_class_cache.csv\")\n",
        "\n",
        "# Load batch CSV\n",
        "df_batch = pd.read_csv(batch_filename)\n",
        "species_names = df_batch[\"GENUS_SPECIES\"].dropna().unique()\n",
        "print(f\"✅ Loaded {len(species_names)} species from {batch_filename}\")\n",
        "\n",
        "# 🔄 Load class cache if it exists\n",
        "if pd.io.common.file_exists(class_cache_file):\n",
        "    cached_df = pd.read_csv(class_cache_file)\n",
        "    tax_class_dict = dict(zip(cached_df[\"GENUS_SPECIES\"], cached_df[\"taxonomic_class\"]))\n",
        "    print(f\"🔄 Loaded {len(tax_class_dict)} classes from cache.\")\n",
        "else:\n",
        "    tax_class_dict = {}\n",
        "\n",
        "to_process = [s for s in species_names if s not in tax_class_dict]\n",
        "print(f\"🔎 {len(to_process)} species left to process.\")"
      ],
      "metadata": {
        "id": "WNSxbW3Pi8zx",
        "outputId": "7bab66d6-e27b-48f5-8a5e-42261d1456d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WNSxbW3Pi8zx",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 100 species from species_batch_002.csv\n",
            "🔎 100 species left to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Species Search Loop"
      ],
      "metadata": {
        "id": "EB3j9upSz_Fo"
      },
      "id": "EB3j9upSz_Fo"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Search Loop of all species in this CSV batch through databases\"\"\"\n",
        "\n",
        "skipped_species_list = []\n",
        "unclassified_species_list = []\n",
        "classified_species_list = []\n",
        "\n",
        "for i, species_name in enumerate(tqdm(to_process, desc=\"Fetching Classes (all DBs)\")):\n",
        "    if \"morphospecies\" in species_name.lower():\n",
        "        tax_class_dict[species_name] = None\n",
        "        unclassified_species_list.append({\"GENUS_SPECIES\": species_name, \"Reason\": \"Morphospecies\"})\n",
        "        continue\n",
        "\n",
        "    # ✅ First: NCBI\n",
        "    class_name = search_term(species_name, db_type=\"NCBI\")\n",
        "\n",
        "    # 🟡 Fallbacks\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"COL\")\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"WORMS\")\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"GBIF\")\n",
        "\n",
        "    tax_class_dict[species_name] = class_name\n",
        "\n",
        "    # Save the unclassified species to a different list!\n",
        "    if class_name:\n",
        "        classified_species_list.append({\"GENUS_SPECIES\": species_name, \"taxonomic_class\": class_name})\n",
        "    else:\n",
        "        unclassified_species_list.append({\"GENUS_SPECIES\": species_name, \"Reason\": \"Not found in any DB\"})\n",
        "\n",
        "\n",
        "\n",
        "    # 💾 Save cache after each species\n",
        "    pd.DataFrame([\n",
        "        {\"GENUS_SPECIES\": k, \"taxonomic_class\": v} for k, v in tax_class_dict.items()\n",
        "    ]).to_csv(class_cache_file, index=False)\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "\"\"\"Create CSVs of the unclassified and classified species and save to GitHub\"\"\"\n",
        "# ✅ Save classified species CSV\n",
        "if classified_species_list:\n",
        "    classified_filename = batch_filename.replace(\".csv\", \"_classified_species.csv\")\n",
        "    pd.DataFrame(classified_species_list).to_csv(classified_filename, index=False)\n",
        "    print(f\"✅ Classified species saved to {classified_filename}\")\n",
        "    setup_ssh_and_push(classified_filename)\n",
        "\n",
        "# ✅ Save unclassified species CSV\n",
        "if unclassified_species_list:\n",
        "    unclassified_filename = batch_filename.replace(\".csv\", \"_unclassified_species.csv\")\n",
        "    pd.DataFrame(unclassified_species_list).to_csv(unclassified_filename, index=False)\n",
        "    print(f\"⚠️ Unclassified species (including morphospecies) saved to {unclassified_filename}\")\n",
        "    setup_ssh_and_push(unclassified_filename)"
      ],
      "metadata": {
        "id": "p8f-TzmxibD_",
        "outputId": "235388c9-9975-43e4-843c-b3044d370868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "p8f-TzmxibD_",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Classes (all DBs):  18%|█▊        | 18/100 [00:06<00:31,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ WoRMS lookup error for Phytoflagellate : 404 Client Error: Not Found for url: https://www.marinespecies.org/rest/AphiaRecordsByName/Phytoflagellate%20?like=false&marine_only=false\n",
            "❌ WoRMS lookup error for Phytoflagellate: Expecting value: line 1 column 1 (char 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Classes (all DBs):  80%|████████  | 80/100 [00:41<00:12,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ WoRMS lookup error for Euglenophycean : 404 Client Error: Not Found for url: https://www.marinespecies.org/rest/AphiaRecordsByName/Euglenophycean%20?like=false&marine_only=false\n",
            "❌ WoRMS lookup error for Euglenophycean: Expecting value: line 1 column 1 (char 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Classes (all DBs): 100%|██████████| 100/100 [00:54<00:00,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Classified species saved to species_batch_002_classified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity/cs3-biodiversity\n",
            "[main 9cc982b] Add batch output species_batch_002_classified_species.csv to biotimes-with-class-label/\n",
            " 1 file changed, 97 insertions(+), 11 deletions(-)\n",
            " rewrite biotimes-with-class-label/species_batch_002_classified_species.csv (91%)\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 1.45 KiB | 1.45 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:emd-aquila/cs3-biodiversity.git\n",
            "   f67fdb6..9cc982b  main -> main\n",
            "/content/cs3-biodiversity\n",
            "⚠️ Unclassified species (including morphospecies) saved to species_batch_002_unclassified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity/cs3-biodiversity\n",
            "[main 6daaaf8] Add batch output species_batch_002_unclassified_species.csv to biotimes-with-class-label/\n",
            " 1 file changed, 5 insertions(+)\n",
            " create mode 100644 biotimes-with-class-label/species_batch_002_unclassified_species.csv\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 492 bytes | 492.00 KiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:emd-aquila/cs3-biodiversity.git\n",
            "   9cc982b..6daaaf8  main -> main\n",
            "/content/cs3-biodiversity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cs3-biodiversity\n",
        "!ssh-agent bash -c 'ssh-add ~/.ssh/id_ed25519; git push origin main'"
      ],
      "metadata": {
        "id": "tg_mYjaA0Qq4",
        "outputId": "96b4e038-416e-486d-ddc0-128db2fa7a01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tg_mYjaA0Qq4",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cs3-biodiversity\n",
            "/root/.ssh/id_ed25519: No such file or directory\n",
            "git@github.com: Permission denied (publickey).\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Collation of Results"
      ],
      "metadata": {
        "id": "LIHugZQJ0D-F"
      },
      "id": "LIHugZQJ0D-F"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd cs3-biodiversity\n",
        "!git pull origin main\n",
        "%cd ..\n",
        "\n",
        "batch_files = sorted(glob.glob(\"cs3-biodiversity/biotimes-with-class-label/species_batch_*_with_class.csv\"))\n",
        "\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
        "combined_df.to_csv(\"all_species_with_class.csv\", index=False)\n",
        "print(\"✅ Combined file saved as all_species_with_class.csv\")\n",
        "\n",
        "push_combined_csv_to_github(\"all_species_with_class.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUxXHIE9LcC",
        "outputId": "3605b36a-44e7-423c-b8ba-6a7633f5c65a"
      },
      "id": "QHUxXHIE9LcC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cs3-biodiversity/cs3-biodiversity\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content/cs3-biodiversity\n",
            "✅ Combined file saved as all_species_with_class.csv\n",
            "✅ Moved all_species_with_class.csv into cs3-biodiversity/biotimes-with-class-label/\n",
            "/content/cs3-biodiversity/cs3-biodiversity\n",
            "[main 72bef5f] Add combined species class CSV to biotimes-with-class-label/\n",
            " 1 file changed, 16 insertions(+)\n",
            " create mode 100644 biotimes-with-class-label/all_species_with_class.csv\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 362 bytes | 362.00 KiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To github.com:emd-aquila/cs3-biodiversity.git\n",
            "   a08521f..72bef5f  main -> main\n",
            "/content/cs3-biodiversity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALTERNATIVE WAY OF COMBINING ALL FILES\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Combine all classified species files\n",
        "classified_files = glob.glob(\"*_classified_species.csv\")\n",
        "if classified_files:\n",
        "    combined_classified = pd.concat([pd.read_csv(f) for f in classified_files], ignore_index=True)\n",
        "    combined_classified.to_csv(\"all_batches_classified_species.csv\", index=False)\n",
        "    print(\"✅ Combined classified species saved to all_batches_classified_species.csv\")\n",
        "else:\n",
        "    print(\"⚠️ No classified species files found.\")\n",
        "\n",
        "# Combine all unclassified species files\n",
        "unclassified_files = glob.glob(\"*_unclassified_species.csv\")\n",
        "if unclassified_files:\n",
        "    combined_unclassified = pd.concat([pd.read_csv(f) for f in unclassified_files], ignore_index=True)\n",
        "    combined_unclassified.to_csv(\"all_batches_unclassified_species.csv\", index=False)\n",
        "    print(\"⚠️ Combined unclassified species saved to all_batches_unclassified_species.csv\")\n",
        "else:\n",
        "    print(\"✅ No unclassified species files found.\")\n"
      ],
      "metadata": {
        "id": "xl65RNEfXv2b"
      },
      "id": "xl65RNEfXv2b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}