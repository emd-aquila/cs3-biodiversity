{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9e16aaf",
      "metadata": {
        "id": "d9e16aaf"
      },
      "source": [
        "# Genus/Species to Class Converter\n",
        "This notebook contains code to convert the genus/species listing from the BioTIME-DB to taxonomic class using Biopython and calling upon the NCBI, COL, GBIF, and WORMS databases, in that order."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Loop Package and Key Setup"
      ],
      "metadata": {
        "id": "lrrHbca8zLWx"
      },
      "id": "lrrHbca8zLWx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages and set NCBI API information.\n",
        "!pip install biopython tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from Bio import Entrez\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from getpass import getpass\n",
        "\n",
        "! wget https://raw.githubusercontent.com/emd-aquila/cs3-biodiversity/main/data/unique_genus_species.csv -O myfile.csv\n",
        "df_all = pd.read_csv(\"myfile.csv\")\n",
        "\n",
        "Entrez.email = \"emduggan@mit.edu\"\n",
        "Entrez.api_key = \"2e5155aba559345711a3af676cb6c6703608\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBDJ-NrRXViX",
        "outputId": "b82ffaaa-a4de-4dd6-d6f3-a69d29b9e503"
      },
      "id": "kBDJ-NrRXViX",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "--2025-05-09 11:07:07--  https://raw.githubusercontent.com/emd-aquila/cs3-biodiversity/main/data/unique_genus_species.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1185895 (1.1M) [text/plain]\n",
            "Saving to: ‘myfile.csv’\n",
            "\n",
            "myfile.csv          100%[===================>]   1.13M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-05-09 11:07:07 (17.8 MB/s) - ‘myfile.csv’ saved [1185895/1185895]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access the private SSH key to upload to GitHub"
      ],
      "metadata": {
        "id": "ZNrK3lB94gu3"
      },
      "id": "ZNrK3lB94gu3"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload id_ed25519 --> under Users/user/.ssh\n",
        "!mkdir -p ~/.ssh\n",
        "!mv id_ed25519 ~/.ssh/\n",
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "!chmod 600 ~/.ssh/id_ed25519\n",
        "!ssh-agent bash -c 'ssh-add ~/.ssh/id_ed25519; ssh -T git@github.com'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "XSsWaR923nFR",
        "outputId": "64cfa6cc-bc3c-4409-d3f0-00d024ded9e5"
      },
      "id": "XSsWaR923nFR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ea15de0-7c2a-4be7-93b0-b6327170715f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ea15de0-7c2a-4be7-93b0-b6327170715f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving id_ed25519 to id_ed25519\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "Identity added: /root/.ssh/id_ed25519 (emduggan@mit.edu)\n",
            "Hi emd-aquila! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create local clone of GitHub repository for file storage"
      ],
      "metadata": {
        "id": "OuwkOnoIzZC6"
      },
      "id": "OuwkOnoIzZC6"
    },
    {
      "cell_type": "code",
      "source": [
        "# if needed to clear existing clone\n",
        "!rm -rf cs3-biodiversity\n",
        "\n",
        "# get access code and clone repo\n",
        "os.environ[\"GITHUB_TOKEN\"] = getpass(\"🔐 Enter your GitHub token: \")\n",
        "token = os.environ[\"GITHUB_TOKEN\"]\n",
        "repo_url = f\"https://emd-aquila:{token}@github.com/emd-aquila/cs3-biodiversity.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcjW9I7KuPMr",
        "outputId": "ceff0a6e-d9e6-44e9-ec50-93d68c3165fb"
      },
      "id": "JcjW9I7KuPMr",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔐 Enter your GitHub token: ··········\n",
            "Cloning into 'cs3-biodiversity'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 148 (delta 77), reused 56 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (148/148), 929.58 KiB | 10.33 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining functions to query database APIs and search for species"
      ],
      "metadata": {
        "id": "rRn4IJJ5zfaU"
      },
      "id": "rRn4IJJ5zfaU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses an email and API key to query the NCBI database\n",
        "def ncbi_query(term):\n",
        "    try:\n",
        "        search = Entrez.esearch(db=\"taxonomy\", term=term, retmode=\"xml\")\n",
        "        result = Entrez.read(search)\n",
        "        if result[\"IdList\"]:\n",
        "            taxid = result[\"IdList\"][0]\n",
        "            fetch = Entrez.efetch(db=\"taxonomy\", id=taxid, retmode=\"xml\")\n",
        "            record = Entrez.read(fetch)[0]\n",
        "            lineage = record.get(\"LineageEx\", [])\n",
        "            class_entry = next((r for r in lineage if r.get(\"Rank\") == \"class\"), None)\n",
        "            return class_entry[\"ScientificName\"] if class_entry else None\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ NCBI lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the Categories of Life database\n",
        "def col_query(term):\n",
        "    url = f\"https://api.catalogueoflife.org/nameusage/search?q={term}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data['total'] > 0:\n",
        "            result = data['result'][0]\n",
        "            lineage = result.get('classification', [])\n",
        "            class_entry = next((r for r in lineage if r.get('rank') == 'class'), None)\n",
        "            return class_entry['name'] if class_entry else None\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ COL lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the WoRMS database\n",
        "def worms_query(term):\n",
        "    url = f\"http://www.marinespecies.org/rest/AphiaRecordsByName/{term}?like=false&marine_only=false\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data:\n",
        "            return data[0].get('class')\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ WoRMS lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# Queries the GBIF Database\n",
        "def gbif_query(term):\n",
        "    url = f\"https://api.gbif.org/v1/species/match?name={term}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data.get(\"class\"):\n",
        "            return data[\"class\"]\n",
        "    except Exception as e:\n",
        "        # print(f\"❌ GBIF lookup error for {term}: {e}\")\n",
        "        return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "bvbqxn05byoo"
      },
      "id": "bvbqxn05byoo",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Searches for the species in the specified DB, first by species and then by genus\n",
        "def search_term(scientific_name, db_type):\n",
        "    \"\"\"\n",
        "    Search for the taxonomic class of a species in a specified database.\n",
        "\n",
        "    Args:\n",
        "        scientific_name (str): The full species name to search.\n",
        "        db_type (str): One of 'NCBI', 'COL', 'WORMS', 'GBIF'.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The class name if found, else None.\n",
        "    \"\"\"\n",
        "\n",
        "    # 🔍 First try full species name\n",
        "    if db_type.upper() == 'NCBI':\n",
        "        result = ncbi_query(scientific_name)\n",
        "    elif db_type.upper() == 'COL':\n",
        "        result = col_query(scientific_name)\n",
        "    elif db_type.upper() == 'GBIF':\n",
        "        result = gbif_query(scientific_name)\n",
        "    elif db_type.upper() == 'WORMS':\n",
        "        result = worms_query(scientific_name)\n",
        "    else:\n",
        "        raise ValueError(\"db_type must be one of 'NCBI', 'COL', 'WORMS', or 'GBIF'.\")\n",
        "\n",
        "    if result:\n",
        "        return result\n",
        "\n",
        "    # 🔄 Fallback: try genus only\n",
        "    genus = scientific_name.split()[0]\n",
        "    if genus != scientific_name:\n",
        "        # print(f\"🔄 Fallback to genus: {genus}\")\n",
        "        if db_type.upper() == 'NCBI':\n",
        "            return ncbi_query(genus)\n",
        "        elif db_type.upper() == 'COL':\n",
        "            return col_query(genus)\n",
        "        elif db_type.upper() == 'WORMS':\n",
        "            return worms_query(genus)\n",
        "        elif db_type.upper() == 'GBIF':\n",
        "            return gbif_query(genus)\n",
        "    return None"
      ],
      "metadata": {
        "id": "No0o6J-liG8P"
      },
      "id": "No0o6J-liG8P",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_ssh_and_push(output_filename,\n",
        "                                 repo_name=\"cs3-biodiversity\",\n",
        "                                 subfolder=\"biotimes-with-class-label\",\n",
        "                                 github_username=\"emd-aquila\",\n",
        "                                 github_email=\"emduggan@mit.edu\",\n",
        "                                 github_name=\"Eli Duggan\"):\n",
        "    \"\"\"\n",
        "    Set up SSH (if needed) and push a batch output file to a subfolder in your GitHub repo using SSH.\n",
        "\n",
        "    Args:\n",
        "        output_filename (str): The CSV filename to push (already saved locally).\n",
        "        repo_name (str): Local folder name of the GitHub repo.\n",
        "        subfolder (str): Subfolder inside the repo to store the file.\n",
        "        github_username (str): Your GitHub username.\n",
        "        github_email (str): Your GitHub email.\n",
        "        github_name (str): Your full name for git config.\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ Add GitHub to known_hosts to prevent verification errors\n",
        "    print(\"🔑 Adding GitHub.com to known_hosts (if not already added)...\")\n",
        "    !mkdir -p ~/.ssh\n",
        "    !ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "\n",
        "    # ✅ Check if repo folder exists, clone if needed\n",
        "    if not os.path.exists(repo_name):\n",
        "        print(f\"📥 Repo folder '{repo_name}' not found. Cloning with SSH...\")\n",
        "        !git clone git@github.com:{github_username}/{repo_name}.git\n",
        "    else:\n",
        "        print(f\"✅ Repo folder '{repo_name}' already exists.\")\n",
        "\n",
        "    # ✅ Create the subfolder inside the repo if it doesn't exist\n",
        "    subfolder_path = f\"{repo_name}/{subfolder}\"\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Copy the batch result into the subfolder\n",
        "    shutil.copy(output_filename, f\"{subfolder_path}/{output_filename}\")\n",
        "    # print(f\"✅ Copied {output_filename} into {subfolder_path}/\")\n",
        "\n",
        "    # ✅ Push to GitHub\n",
        "    %cd {repo_name}\n",
        "\n",
        "    # Set Git config (only needed once)\n",
        "    !git config user.email \"{github_email}\"\n",
        "    !git config user.name \"{github_name}\"\n",
        "\n",
        "    # Set remote to SSH (just in case)\n",
        "    !git remote set-url origin git@github.com:{github_username}/{repo_name}.git\n",
        "    !git pull origin main --no-edit --rebase=false\n",
        "\n",
        "    # Add, commit, push\n",
        "    !git add {subfolder}/{output_filename}\n",
        "    !git commit -m \"Add batch output {output_filename} to {subfolder}/\" || echo \"No changes to commit.\"\n",
        "    !git push origin main\n",
        "\n",
        "    # ✅ Return to root directory\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "c2txzoCM49oD"
      },
      "id": "c2txzoCM49oD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_combined_csv_to_github(combined_filename=\"all_species_with_class.csv\",\n",
        "                                repo_name=\"cs3-biodiversity\",\n",
        "                                subfolder=\"biotimes-with-class-label\",\n",
        "                                github_username=\"emd-aquila\",\n",
        "                                github_email=\"emduggan@mit.edu\",\n",
        "                                github_name=\"Eli Duggan\"):\n",
        "    \"\"\"\n",
        "    Push the combined CSV file to a subfolder in your GitHub repo using SSH.\n",
        "    \"\"\"\n",
        "    # ✅ Build the full subfolder path\n",
        "    subfolder_path = f\"{repo_name}/{subfolder}\"\n",
        "\n",
        "    # ✅ Make sure the subfolder exists inside the repo\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Move the combined CSV into the subfolder\n",
        "    dest_path = f\"{subfolder_path}/{combined_filename}\"\n",
        "    if not os.path.exists(dest_path):\n",
        "        shutil.move(combined_filename, dest_path)\n",
        "        print(f\"✅ Moved {combined_filename} into {subfolder_path}/\")\n",
        "    else:\n",
        "        print(f\"✅ {combined_filename} already exists in {subfolder_path}/\")\n",
        "\n",
        "    # ✅ Push to GitHub\n",
        "    %cd {repo_name}\n",
        "\n",
        "    !git config user.email \"{github_email}\"\n",
        "    !git config user.name \"{github_name}\"\n",
        "    !git remote set-url origin git@github.com:{github_username}/{repo_name}.git\n",
        "    !git pull origin main --no-edit --rebase=false\n",
        "\n",
        "    !git add {subfolder}/{combined_filename}\n",
        "    !git commit -m \"Add combined species class CSV to {subfolder}/\" || echo \"No changes to commit.\"\n",
        "    !git push origin main\n",
        "\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "R4cS-WGh-Jt2"
      },
      "id": "R4cS-WGh-Jt2",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching Data and Loading the Cache"
      ],
      "metadata": {
        "id": "_6IvUjjRzmo6"
      },
      "id": "_6IvUjjRzmo6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching data and saving\n"
      ],
      "metadata": {
        "id": "Se85TdQFzs_x"
      },
      "id": "Se85TdQFzs_x"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8000\n",
        "num_batches = math.ceil(len(df_all) / batch_size)\n",
        "\n",
        "# Save each batch\n",
        "for i in range(num_batches):\n",
        "    batch_df = df_all.iloc[i*batch_size : (i+1)*batch_size]\n",
        "    batch_file = f\"species_batch_{i+1:03d}.csv\"\n",
        "    batch_df.to_csv(batch_file, index=False)\n",
        "    # print(f\"Saved {batch_file}\")"
      ],
      "metadata": {
        "id": "3UWDlYsyiyMU",
        "collapsed": true
      },
      "id": "3UWDlYsyiyMU",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Batch of Interest"
      ],
      "metadata": {
        "id": "B9ZkU8G0399J"
      },
      "id": "B9ZkU8G0399J"
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_filename = \"species_batch_001.csv\" #DONE\n",
        "# batch_filename = \"species_batch_002.csv\" #DONE\n",
        "# batch_filename = \"species_batch_003.csv\" #DONE\n",
        "# batch_filename = \"species_batch_004.csv\" #DONE\n",
        "# batch_filename = \"species_batch_005.csv\" #DONE\n",
        "batch_filename = \"species_batch_006.csv\" #TODO"
      ],
      "metadata": {
        "id": "U-t-m9R639FA"
      },
      "id": "U-t-m9R639FA",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear cache (if needed)"
      ],
      "metadata": {
        "id": "wpjplRDuz6G0"
      },
      "id": "wpjplRDuz6G0"
    },
    {
      "cell_type": "code",
      "source": [
        "class_cache_file = batch_filename.replace(\".csv\", \"_class_cache.csv\")\n",
        "\n",
        "# Delete class cache\n",
        "if os.path.exists(class_cache_file):\n",
        "    os.remove(class_cache_file)\n",
        "    print(f\"🗑️ Deleted {class_cache_file}\")\n",
        "else:\n",
        "    print(f\"⚠️ No cache file found for {class_cache_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4jvsfJures6",
        "outputId": "f1299dee-19de-4dfc-d0f4-82e6e31491f2"
      },
      "id": "Q4jvsfJures6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No cache file found for species_batch_006_class_cache.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading batch files and caching files (run before loop)"
      ],
      "metadata": {
        "id": "lF0gQd8Wzwlt"
      },
      "id": "lF0gQd8Wzwlt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache files\n",
        "taxid_cache_file = batch_filename.replace(\".csv\", \"_taxid_cache.csv\")\n",
        "class_cache_file = batch_filename.replace(\".csv\", \"_class_cache.csv\")\n",
        "\n",
        "# Load batch CSV\n",
        "df_batch = pd.read_csv(batch_filename)\n",
        "species_names = df_batch[\"GENUS_SPECIES\"].dropna().unique()\n",
        "print(f\"✅ Loaded {len(species_names)} species from {batch_filename}\")\n",
        "\n",
        "# 🔄 Load class cache if it exists\n",
        "if pd.io.common.file_exists(class_cache_file):\n",
        "    cached_df = pd.read_csv(class_cache_file)\n",
        "    tax_class_dict = dict(zip(cached_df[\"GENUS_SPECIES\"], cached_df[\"taxonomic_class\"]))\n",
        "    print(f\"🔄 Loaded {len(tax_class_dict)} classes from cache.\")\n",
        "else:\n",
        "    tax_class_dict = {}\n",
        "\n",
        "to_process = [s for s in species_names if s not in tax_class_dict]\n",
        "print(f\"🔎 {len(to_process)} species left to process.\")"
      ],
      "metadata": {
        "id": "WNSxbW3Pi8zx",
        "outputId": "578afd05-51ed-41ec-e8da-5f2c6b10f51d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WNSxbW3Pi8zx",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 1852 species from species_batch_006.csv\n",
            "🔎 1852 species left to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Species Search Loop"
      ],
      "metadata": {
        "id": "EB3j9upSz_Fo"
      },
      "id": "EB3j9upSz_Fo"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Search Loop of all species in this CSV batch through databases\"\"\"\n",
        "\n",
        "skipped_species_list = []\n",
        "unclassified_species_list = []\n",
        "classified_species_list = []\n",
        "\n",
        "for i, species_name in enumerate(tqdm(to_process, desc=\"Fetching Classes (all DBs)\")):\n",
        "    if \"morphospecies\" in species_name.lower():\n",
        "        tax_class_dict[species_name] = None\n",
        "        unclassified_species_list.append({\"GENUS_SPECIES\": species_name, \"Reason\": \"Morphospecies\"})\n",
        "        continue\n",
        "\n",
        "    # ✅ First: NCBI\n",
        "    class_name = search_term(species_name, db_type=\"NCBI\")\n",
        "\n",
        "    # 🟡 Fallbacks\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"COL\")\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"WORMS\")\n",
        "    if not class_name:\n",
        "        class_name = search_term(species_name, db_type=\"GBIF\")\n",
        "\n",
        "    tax_class_dict[species_name] = class_name\n",
        "\n",
        "    # Save the unclassified species to a different list!\n",
        "    if class_name:\n",
        "        classified_species_list.append({\"GENUS_SPECIES\": species_name, \"taxonomic_class\": class_name})\n",
        "    else:\n",
        "        unclassified_species_list.append({\"GENUS_SPECIES\": species_name, \"Reason\": \"Not found in any DB\"})\n",
        "\n",
        "\n",
        "\n",
        "    # 💾 Save cache after each species\n",
        "    pd.DataFrame([\n",
        "        {\"GENUS_SPECIES\": k, \"taxonomic_class\": v} for k, v in tax_class_dict.items()\n",
        "    ]).to_csv(class_cache_file, index=False)\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "\n",
        "\"\"\"Adding previously cached species\"\"\"\n",
        "# Load the full cache\n",
        "full_cache_df = pd.read_csv(class_cache_file)\n",
        "\n",
        "# Build a set of species that have already been included (from the current batch)\n",
        "already_included_species = set([entry[\"GENUS_SPECIES\"] for entry in classified_species_list] +\n",
        "                               [entry[\"GENUS_SPECIES\"] for entry in unclassified_species_list])\n",
        "\n",
        "# Loop over all cached species\n",
        "for _, row in full_cache_df.iterrows():\n",
        "    species_name = row[\"GENUS_SPECIES\"]\n",
        "    class_name = row[\"taxonomic_class\"]\n",
        "\n",
        "    if species_name in already_included_species:\n",
        "        continue  # Already added during this batch\n",
        "\n",
        "    if pd.isna(class_name) or class_name in [None, '', 'nan']:\n",
        "        unclassified_species_list.append({\"GENUS_SPECIES\": species_name, \"Reason\": \"Not found in any DB\"})\n",
        "    else:\n",
        "        classified_species_list.append({\"GENUS_SPECIES\": species_name, \"taxonomic_class\": class_name})\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Create CSVs of the unclassified and classified species and save to GitHub\"\"\"\n",
        "# ✅ Save classified species CSV\n",
        "if classified_species_list:\n",
        "    classified_filename = batch_filename.replace(\".csv\", \"_classified_species.csv\")\n",
        "    pd.DataFrame(classified_species_list).to_csv(classified_filename, index=False)\n",
        "    print(f\"✅ Classified species saved to {classified_filename}\")\n",
        "    setup_ssh_and_push(classified_filename)\n",
        "\n",
        "# ✅ Save unclassified species CSV\n",
        "if unclassified_species_list:\n",
        "    unclassified_filename = batch_filename.replace(\".csv\", \"_unclassified_species.csv\")\n",
        "    pd.DataFrame(unclassified_species_list).to_csv(unclassified_filename, index=False)\n",
        "    print(f\"⚠️ Unclassified species (including morphospecies) saved to {unclassified_filename}\")\n",
        "    setup_ssh_and_push(unclassified_filename)"
      ],
      "metadata": {
        "id": "p8f-TzmxibD_",
        "outputId": "f6dd4301-5f57-442f-850f-afeca7bf0470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "p8f-TzmxibD_",
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching Classes (all DBs): 100%|██████████| 1852/1852 [12:27<00:00,  2.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Classified species saved to species_batch_006_classified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "[main f3e47ea] Add batch output species_batch_006_classified_species.csv to biotimes-with-class-label/\n",
            " 1 file changed, 1852 insertions(+)\n",
            " create mode 100644 biotimes-with-class-label/species_batch_006_classified_species.csv\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 16.32 KiB | 5.44 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:emd-aquila/cs3-biodiversity.git\n",
            "   63f211e..f3e47ea  main -> main\n",
            "/content\n",
            "⚠️ Unclassified species (including morphospecies) saved to species_batch_006_unclassified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "[main e019822] Add batch output species_batch_006_unclassified_species.csv to biotimes-with-class-label/\n",
            " 1 file changed, 2 insertions(+)\n",
            " create mode 100644 biotimes-with-class-label/species_batch_006_unclassified_species.csv\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (4/4), 409 bytes | 409.00 KiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:emd-aquila/cs3-biodiversity.git\n",
            "   f3e47ea..e019822  main -> main\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining batch CSV results and pushing to GitHub\n"
      ],
      "metadata": {
        "id": "LIHugZQJ0D-F"
      },
      "id": "LIHugZQJ0D-F"
    },
    {
      "cell_type": "code",
      "source": [
        "# ALTERNATIVE WAY OF COMBINING ALL FILES\n",
        "\n",
        "repo_name = \"cs3-biodiversity\"\n",
        "subfolder = \"biotimes-with-class-label\"\n",
        "\n",
        "# ✅ 1️⃣ Make sure repo is cloned and updated\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"📥 Cloning repo {repo_name}...\")\n",
        "    !git clone git@github.com:emd-aquila/{repo_name}.git\n",
        "else:\n",
        "    print(f\"✅ Repo {repo_name} already exists. Pulling latest changes...\")\n",
        "    os.chdir(repo_name)\n",
        "    !git pull origin main\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "\n",
        "\"\"\"Classified Species Files\"\"\"\n",
        "classified_files = [\n",
        "    f for f in glob.glob(f\"{repo_name}/{subfolder}/*_classified_species.csv\")\n",
        "    if \"all_classified_species\" not in os.path.basename(f)\n",
        "]\n",
        "if classified_files:\n",
        "    print(f\"Combining {len(classified_files)} classified files:\")\n",
        "    for f in classified_files:\n",
        "        print(f)\n",
        "\n",
        "    combined_classified = pd.concat([pd.read_csv(f) for f in classified_files], ignore_index=True)\n",
        "\n",
        "    # ✅ Sort alphabetically by species name\n",
        "    combined_classified = combined_classified.sort_values(by=\"GENUS_SPECIES\", ascending=True)\n",
        "    combined_classified.to_csv(\"all_classified_species.csv\", index=False)\n",
        "    setup_ssh_and_push(\"all_classified_species.csv\")\n",
        "else:\n",
        "    print(\"⚠️ No classified species files found in the repo.\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Unclassified Species Files\"\"\"\n",
        "unclassified_files = [\n",
        "    f for f in glob.glob(f\"{repo_name}/{subfolder}/*_unclassified_species.csv\")\n",
        "    if \"all_unclassified_species\" not in os.path.basename(f)\n",
        "]\n",
        "if unclassified_files:\n",
        "    print(f\"Combining {len(unclassified_files)} unclassified files:\")\n",
        "    for f in unclassified_files:\n",
        "        print(f)\n",
        "\n",
        "    combined_unclassified = pd.concat([pd.read_csv(f) for f in unclassified_files], ignore_index=True)\n",
        "\n",
        "    # ✅ Sort alphabetically by species name\n",
        "    combined_unclassified = combined_unclassified.sort_values(by=\"GENUS_SPECIES\", ascending=True)\n",
        "    combined_unclassified.to_csv(\"all_unclassified_species.csv\", index=False)\n",
        "    setup_ssh_and_push(\"all_unclassified_species.csv\")\n",
        "else:\n",
        "    print(\"⚠️ No unclassified species files found in the repo.\")\n"
      ],
      "metadata": {
        "id": "xl65RNEfXv2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c64d38-8c20-4d15-aa5a-ef07c813e967"
      },
      "id": "xl65RNEfXv2b",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Repo cs3-biodiversity already exists. Pulling latest changes...\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Combining 6 classified files:\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_006_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_004_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_005_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_001_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_003_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_002_classified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "No changes to commit.\n",
            "Everything up-to-date\n",
            "/content\n",
            "Combining 6 unclassified files:\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_002_unclassified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_006_unclassified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_003_unclassified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_001_unclassified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_004_unclassified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_005_unclassified_species.csv\n",
            "🔑 Adding GitHub.com to known_hosts (if not already added)...\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "# github.com:22 SSH-2.0-0264bb16\n",
            "✅ Repo folder 'cs3-biodiversity' already exists.\n",
            "/content/cs3-biodiversity\n",
            "From github.com:emd-aquila/cs3-biodiversity\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "No changes to commit.\n",
            "Everything up-to-date\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Found {len(classified_files)} classified CSV files:\")\n",
        "for f in classified_files:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "GQr50nOrryQL",
        "outputId": "e5a267f5-4da7-4e17-ec92-0ccb663db315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GQr50nOrryQL",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 classified CSV files:\n",
            "cs3-biodiversity/biotimes-with-class-label/all_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_006_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_004_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_005_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_001_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_003_classified_species.csv\n",
            "cs3-biodiversity/biotimes-with-class-label/species_batch_002_classified_species.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}